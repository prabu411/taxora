from transformers import pipeline
import torch
import sys
import os

print("ğŸš€ Testing Lightweight Financial AI Model")
print("=" * 50)

# Use a small, fast model optimized for conversations (~350MB)
MODEL_ID = "microsoft/DialoGPT-medium"

print(f"ğŸ“¦ Loading model: {MODEL_ID}")
print("â³ Small download (~350MB) - much faster than 7GB!")

try:
    # Create conversational AI pipeline
    print("ğŸ”§ Setting up AI pipeline...")

    chatbot = pipeline(
        "conversational",
        model=MODEL_ID,
        device=-1,  # Use CPU for compatibility
        return_full_text=False
    )

    print("âœ… Lightweight AI model ready!")
    print("ğŸ’¾ Model size: ~350MB (vs 7GB for full Granite)")
    print("âš¡ Fast inference on CPU")

    # Test financial conversations
    from transformers import Conversation

    print("\n" + "="*50)
    print("ğŸ’¬ TESTING FINANCIAL AI RESPONSES")
    print("="*50)

    # Test 1: Investment advice
    conv1 = Conversation("I have $1000 to invest. What are some safe options for a beginner?")
    result1 = chatbot(conv1)

    print(f"\nğŸ‘¤ User: {conv1.messages[0]['content']}")
    print(f"ğŸ¤– AI: {result1.messages[-1]['content']}")

    # Test 2: Budgeting
    conv2 = Conversation("How should I create a monthly budget as a college student?")
    result2 = chatbot(conv2)

    print(f"\nğŸ‘¤ User: {conv2.messages[0]['content']}")
    print(f"ğŸ¤– AI: {result2.messages[-1]['content']}")

    # Test 3: Savings
    conv3 = Conversation("What's the best way to start an emergency fund?")
    result3 = chatbot(conv3)

    print(f"\nğŸ‘¤ User: {conv3.messages[0]['content']}")
    print(f"ğŸ¤– AI: {result3.messages[-1]['content']}")

    print("\n" + "="*50)
    print("âœ… LIGHTWEIGHT AI TEST SUCCESSFUL!")
    print("ğŸ‰ Ready to integrate with your Taxora chatbot!")
    print("ğŸ“Š Performance: Fast responses, low memory usage")
    print("ğŸ’¡ Perfect for real-time financial advice")

except Exception as e:
    print(f"\nâŒ Error: {e}")
    print("\nğŸ”§ Trying even smaller model...")

    try:
        # Fallback to tiny model
        MODEL_ID = "microsoft/DialoGPT-small"  # Only ~117MB
        print(f"ğŸ“¦ Loading backup model: {MODEL_ID} (~117MB)")

        chatbot = pipeline("conversational", model=MODEL_ID, device=-1)

        conv = Conversation("Tell me about saving money")
        result = chatbot(conv)

        print(f"ğŸ‘¤ User: {conv.messages[0]['content']}")
        print(f"ğŸ¤– AI: {result.messages[-1]['content']}")
        print("âœ… Backup model working!")

    except Exception as e2:
        print(f"âŒ Backup failed: {e2}")
        print("ğŸ’¡ Please check internet connection and try again")
    # Use pipeline for easier setup - much faster and smaller
    print("ğŸ”¤ Creating AI pipeline...")
    print("ğŸ“¦ Downloading lightweight model (~117MB)...")

    # Create a conversational pipeline
    chatbot = pipeline(
        "conversational",
        model=MODEL_ID,
        tokenizer=MODEL_ID,
        device=0 if torch.cuda.is_available() else -1,
        framework="pt"
    )

    print("âœ… Lightweight AI model loaded successfully!")
    print(f"ğŸ’¾ Model size: ~117MB (much smaller than 7GB!)")
    print(f"ğŸ–¥ï¸  Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")

    # Test with financial advice
    from transformers import Conversation

    conversation = Conversation("I have $1000 to invest. What are some safe investment options for a beginner?")

    print("\nğŸ’¬ Testing with financial advice prompt:")
    print(f"ğŸ“ User: {conversation.messages[0]['content']}")
    print("\nğŸ¤– AI Response:")
    print("-" * 40)

    # Generate response
    result = chatbot(conversation)
    response = result.messages[-1]['content']

    print(f"ğŸ’¡ {response}")

    # Test another financial question
    print("\n" + "-" * 40)
    print("ğŸ”„ Testing another question...")

    conversation.add_user_input("What about budgeting tips for someone just starting their career?")
    result = chatbot(conversation)
    response = result.messages[-1]['content']

    print(f"ğŸ“ User: What about budgeting tips for someone just starting their career?")
    print(f"ğŸ’¡ AI: {response}")

    print("\n" + "=" * 50)
    print("âœ… IBM Granite test completed successfully!")
    print("ğŸ‰ Your model is working and ready for integration!")

except Exception as e:
    print(f"\nâŒ Error testing IBM Granite model: {e}")
    print("\nğŸ”§ Troubleshooting tips:")
    print("1. Make sure PyTorch is fully installed")
    print("2. Check your internet connection for model download")
    print("3. Ensure you have enough disk space (model is ~2GB)")
    print("4. Try running: pip install --upgrade transformers torch")
    sys.exit(1)
